{
    "tokenized": [
        "my",
        "\u261d",
        "\ud83e\udd22",
        "\ud83d\udc4e",
        "landlord",
        "said",
        "\ud83d\udde3",
        "no",
        "pets",
        "\ud83d\udeab",
        "\ud83d\udc36",
        "\ud83d\udeab",
        "\ud83d\udc31",
        "but",
        "my",
        "pet",
        "\ud83d\udc36",
        "said",
        "\ud83d\udde3",
        "\u274c",
        "no",
        "\u274c",
        "landlords",
        "so",
        "guess",
        "which",
        "\u261d",
        "one",
        "is",
        "getting",
        "euthanized",
        "\u2754",
        "\ud83d\ude0c"
    ]
}